{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c33126c",
   "metadata": {},
   "source": [
    "# Aymen-GPT notebook\n",
    "\n",
    "***This notebook, comes as complementary for the Aymen-GPT model who is a from scratch implementation of the GPT-2 model (124M) inspired from both GPT-2 and GPT-3 architectures.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d3086",
   "metadata": {},
   "source": [
    "### **Simple GPT-2 text generation from huggingface transformers library**\n",
    "\n",
    "GPT-2 text generation from scratch as example using the huggingface transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e173535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example implementing the text generation with the GPT2 124M model from huggingface transformers library.\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\") # 124M\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "tokens = [15496, 11, 314, 1101, 257, 627, 1767, 1110, 11] # \"Hello, I'm a data scientist,\"\n",
    "tokens = torch.tensor(tokens, dtype=torch.long) # (8,)\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1) # (5, 8)\n",
    "x = tokens.to('cuda')\n",
    "\n",
    "# generate\n",
    "while x.size(1) < 30: # max_length=30\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0] # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :] # (B, vocab_size)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # top-k sampling of 50 \n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        # select a token from the top-k probabilities\n",
    "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
    "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "for i in range(5):\n",
    "    tokens = x[i, :30].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814f420",
   "metadata": {},
   "source": [
    "### **Learning Rate Schedule Visualization**\n",
    "\n",
    "This cell visualizes the learning rate schedule used in Aymen-GPT training based on the GPT-3 paper. The schedule consists of:\n",
    "1. A linear warmup phase\n",
    "2. A cosine decay phase\n",
    "3. A constant minimum learning rate phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e59986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib as mpl\n",
    "\n",
    "style.use('seaborn-v0_8-whitegrid')\n",
    "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "mpl.rcParams['font.size'] = 11\n",
    "mpl.rcParams['axes.titlesize'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 10\n",
    "mpl.rcParams['ytick.labelsize'] = 10\n",
    "\n",
    "# Training parameters from your GPT implementation\n",
    "warmup_steps = 715\n",
    "max_steps = 19073\n",
    "min_lr = 6e-5  # 10% of max_lr as per your code\n",
    "max_lr = 6e-4\n",
    "\n",
    "# Generate x-axis values (training steps)\n",
    "steps = np.linspace(0, max_steps + 1000, 1000)  # +1000 to show final behavior\n",
    "\n",
    "# Implement learning rate schedule from your code\n",
    "lr_values = []\n",
    "for it in steps:\n",
    "    # 1) linear warmup for warmup_steps\n",
    "    if it < warmup_steps:\n",
    "        lr = max_lr * (it / warmup_steps)\n",
    "    # 2) if it > max_steps, return min learning rate\n",
    "    elif it > max_steps:\n",
    "        lr = min_lr\n",
    "    # 3) cosine decay from max_lr to min_lr\n",
    "    else:\n",
    "        decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
    "        coeff = 0.5 * (1.0 + np.cos(np.pi * decay_ratio))\n",
    "        lr = min_lr + coeff * (max_lr - min_lr)\n",
    "    lr_values.append(lr)\n",
    "\n",
    "# Convert to numpy for easier handling\n",
    "lr_values = np.array(lr_values)\n",
    "\n",
    "# Create the plot with training phases highlighted\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the learning rate curve\n",
    "ax.plot(steps, lr_values, lw=2.5, color='#2A6EB9')\n",
    "\n",
    "# Highlight the different training phases\n",
    "ax.axvspan(0, warmup_steps, alpha=0.2, color='#FFA500', label='Warmup Phase')\n",
    "ax.axvspan(warmup_steps, max_steps, alpha=0.2, color='#4CAF50', label='Cosine Decay')\n",
    "ax.axvspan(max_steps, max(steps), alpha=0.2, color='#F06292', label='Min LR')\n",
    "\n",
    "# Add vertical lines at transition points\n",
    "ax.axvline(x=warmup_steps, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.axvline(x=max_steps, color='gray', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add annotations\n",
    "ax.annotate('Linear Warmup', xy=(max_steps/14.5, max_lr/1.3), \n",
    "            ha='center', va='center', fontsize=10)\n",
    "ax.annotate('Cosine Decay', xy=((warmup_steps + max_steps)/2.2, max_lr/2),\n",
    "            ha='center', va='center', fontsize=10)\n",
    "ax.annotate('Constant Min LR', xy=(max_steps + 500, min_lr*1.2),\n",
    "            ha='left', va='center', fontsize=10)\n",
    "\n",
    "# Format the chart\n",
    "ax.set_title('Aymen-GPT Training: Learning Rate Schedule', fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Training Steps', fontweight='semibold', labelpad=10)\n",
    "ax.set_ylabel('Learning Rate', fontweight='semibold', labelpad=10)\n",
    "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.set_xlim(0, max(steps))\n",
    "ax.set_ylim(0, max_lr*1.05)\n",
    "\n",
    "# Add data points for key transitions\n",
    "ax.plot(0, 0, 'o', markersize=6, color='red')\n",
    "ax.plot(warmup_steps, max_lr, 'o', markersize=6, color='red')\n",
    "ax.plot(max_steps, min_lr, 'o', markersize=6, color='red')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='upper right', frameon=True)\n",
    "\n",
    "# Show percentage of max learning rate on right y-axis\n",
    "ax_right = ax.twinx()\n",
    "ax_right.set_ylim(0, 100)\n",
    "ax_right.yaxis.set_major_formatter(PercentFormatter())\n",
    "ax_right.set_ylabel('Percentage of Maximum LR', fontweight='semibold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d6f74",
   "metadata": {},
   "source": [
    "### **Models evaluation and comparison**\n",
    "\n",
    "**(On the right)** We will visualize the training loss and validation loss for Aymen-GPT on the fineweb-edu dataset (10B*4 tokens) in 4 epochs , compared to the baseline model GPT-2 (124M) checkpoint validation loss.\n",
    "\n",
    "**(On the left)** We will visualize the Aymen-GPT Hellaswag evaluation accuracy compared to the baseline models GPT-2 (124M) and GPT-3 (124M trained on 300B tokens) Hellaswag evaluation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9350f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse and visualize the logfile\n",
    "%matplotlib inline\n",
    "\n",
    "sz = \"124M\"\n",
    "\n",
    "loss_baseline = {\n",
    "    \"124M\": 3.2924,\n",
    "}[sz]\n",
    "hella2_baseline = { # HellaSwag for GPT-2\n",
    "    \"124M\": 0.294463,\n",
    "    \"350M\": 0.375224,\n",
    "    \"774M\": 0.431986,\n",
    "    \"1558M\": 0.488946,\n",
    "}[sz]\n",
    "hella3_baseline = { # HellaSwag for GPT-3\n",
    "    \"124M\": 0.337,\n",
    "    \"350M\": 0.436,\n",
    "    \"774M\": 0.510,\n",
    "    \"1558M\": 0.547,\n",
    "}[sz]\n",
    "\n",
    "# load the log file\n",
    "with open(\"log124M_40B/log.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# parse the individual lines, group by stream (train,val,hella)\n",
    "streams = {}\n",
    "for line in lines:\n",
    "    step, stream, val = line.strip().split()\n",
    "    if stream not in streams:\n",
    "        streams[stream] = {}\n",
    "    streams[stream][int(step)] = float(val)\n",
    "\n",
    "# convert each stream from {step: val} to (steps[], vals[])\n",
    "# so it's easier for plotting\n",
    "streams_xy = {}\n",
    "for k, v in streams.items():\n",
    "    # get all (step, val) items, sort them\n",
    "    xy = sorted(list(v.items()))\n",
    "    # unpack the list of tuples to tuple of lists\n",
    "    streams_xy[k] = list(zip(*xy))\n",
    "\n",
    "# create figure\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Panel 1: losses: both train and val\n",
    "plt.subplot(121)\n",
    "xs, ys = streams_xy[\"train\"] # training loss\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f'Aymen-GPT ({sz}) train loss')\n",
    "print(\"Min Train Loss:\", min(ys))\n",
    "xs, ys = streams_xy[\"val\"] # validation loss\n",
    "plt.plot(xs, ys, label=f'Aymen-GPT ({sz}) val loss')\n",
    "# horizontal line at GPT-2 baseline\n",
    "if loss_baseline is not None:\n",
    "    plt.axhline(y=loss_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint val loss\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(top=4.0)\n",
    "plt.legend()\n",
    "plt.title(\"Loss\")\n",
    "print(\"Min Validation Loss:\", min(ys))\n",
    "\n",
    "# Panel 2: HellaSwag eval\n",
    "plt.subplot(122)\n",
    "xs, ys = streams_xy[\"hella\"] # HellaSwag eval\n",
    "ys = np.array(ys)\n",
    "plt.plot(xs, ys, label=f\"Aymen-GPT ({sz})\")\n",
    "# horizontal line at GPT-2 baseline\n",
    "if hella2_baseline:\n",
    "    plt.axhline(y=hella2_baseline, color='r', linestyle='--', label=f\"OpenAI GPT-2 ({sz}) checkpoint\")\n",
    "if hella3_baseline:\n",
    "    plt.axhline(y=hella3_baseline, color='g', linestyle='--', label=f\"OpenAI GPT-3 ({sz}) checkpoint\")\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"HellaSwag eval\")\n",
    "print(\"Max Hellaswag eval:\", max(ys))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
